{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2004a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d90125",
   "metadata": {},
   "source": [
    "## Parse the webpage and extract hyperlinks\n",
    "\n",
    "Make a table for each webpage where you see the position of the link and the mention frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024fcc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse(url):\n",
    "    response = requests.get(url) # send GET request to URL\n",
    "    html_content = response.content #get the HTML content\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")  # parse html using soup\n",
    "    hyperlinks = soup.find_all(\"a\")  # Find all the hyperlinks\n",
    "    names = [] # make 4 empty lists\n",
    "    positions = []\n",
    "    num_mentions_list = []\n",
    "    urls = []\n",
    "    for hyperlink in hyperlinks:       # loop each link and extract 4 pieces of info\n",
    "        name = hyperlink.text.strip()          # get name of hyperlink\n",
    "        position = hyperlink.parent.text.index(name)      # get position of hyperlink in text\n",
    "        num_mentions = hyperlink.parent.text.count(name)   # get number of mentions in text\n",
    "        href = hyperlink.get('href')  # get the href url\n",
    "        full_url = urljoin(url, href)  # join base and relative url together\n",
    "        names.append(name) # append each of 4 lists together\n",
    "        positions.append(position)\n",
    "        num_mentions_list.append(num_mentions)\n",
    "        urls.append(full_url)\n",
    "    data = {                  # make it into a dataframe!\n",
    "        \"name\": names,\n",
    "        \"position\": positions,\n",
    "        \"mentions\": num_mentions_list,\n",
    "        \"url\": urls}\n",
    "    df = pd.DataFrame(data) #into a df!\n",
    "    df = df.sort_values('mentions', ascending=False) \n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def _delete(df):\n",
    "    df = df[df.name.str.strip().astype(bool)]  # this will remove rows where the Name is empty\n",
    "    df = df[df.name.str.len() > 1]  # this will remove the rows where Name is a single character\n",
    "    df = df[df.url.str.contains('Special') == False]\n",
    "    df = df[df.mentions >= 2].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3748b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = _parse('https://en.wikipedia.org/wiki/Semantic_similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29991cf0",
   "metadata": {},
   "source": [
    "## Iterate for child articles\n",
    "Repeat the parsing process for the mentioned articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb68a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "prioritised = _delete(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _function(mother, child):\n",
    "    response = requests.get(child)\n",
    "    html_content = response.content\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    try:\n",
    "        text = soup.get_text()\n",
    "        positioning = text.index(mother)\n",
    "        mentioning = text.count(mother)\n",
    "        return positioning, mentioning\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prioritised[['positioning', 'mentioning']] = prioritised.apply(lambda row: pd.Series(_function(row['name'], row['url'])), axis=1).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prioritised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ff094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
